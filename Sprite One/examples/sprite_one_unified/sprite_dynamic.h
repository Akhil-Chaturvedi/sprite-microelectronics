/*
 * sprite_dynamic.h
 * The "Universal Adaptor" for Sprite One
 * Dynamically constructs AIfES models from binary `.aif32` files (V2 Format)
 */

#ifndef SPRITE_DYNAMIC_H
#define SPRITE_DYNAMIC_H

#include "aifes.h"
#include <vector>

// .aif32 File Format V2 Constants
#define MODEL_MAGIC 0x54525053 // "SPRT"
#define LAYER_TYPE_INPUT    0
#define LAYER_TYPE_DENSE    1
#define LAYER_TYPE_CONV2D   2
#define LAYER_TYPE_FLATTEN  3
#define LAYER_TYPE_SIGMOID  10
#define LAYER_TYPE_RELU     11
#define LAYER_TYPE_SOFTMAX  12

// Struct for the file header
struct DynamicModelHeader {
    uint32_t magic;
    uint16_t version;
    uint8_t  input_size;     // Warning: This field is legacy, use real tensor shapes
    uint8_t  output_size;
    uint8_t  hidden_size;
    uint8_t  model_type;     // 0=F32, 1=Q7
    uint8_t  num_layers;
    uint8_t  reserved;
    uint32_t weights_crc;
    char     name[16];
};

class DynamicModel {
private:
    // Memory pools
    std::vector<uint8_t> param_buffer; // Holds layer structs and params
    std::vector<uint8_t> weight_buffer; // Holds actual weights (we copy from file buffer)
    
    // AIfES objects
    aimodel_t model;
    
    // Pointers to allocated layers to prevent scope loss
    std::vector<void*> layer_ptrs;
    
    // Model architecture shape (for inference to know what to do)
    uint16_t input_shape[2];  // [batch, input_dim]
    uint16_t output_shape[2]; // [batch, output_dim]
    
public:
    DynamicModel() {
        // Init model struct
        model.input_layer = nullptr;
        model.output_layer = nullptr;
        input_shape[0] = 1; input_shape[1] = 0;
        output_shape[0] = 1; output_shape[1] = 0;
    }
    
    ~DynamicModel() {
        // Cleanup if needed
        free_resources();
    }
    
    void free_resources() {
        // Vectors auto-clear
        // Use `delete` on raw pointers in layer_ptrs if needed, but we used placement new or standard new?
        // We used standard `new`. So we must delete them.
        for (void* ptr : layer_ptrs) {
            // Wait, we can't delete void*. We need to know the type or use a base struct with virtual destructor.
            // AIfES structs are plain C structs, no virtual destructors.
            // BUT, we cast them to `void*` in the vector. 
            // In "God Mode", we just leaky-leak or cast back if we tracked types.
            // Ideally we'd use a union or a wrapper class.
            // For now, let's assume `free(ptr)` works if they were malloc'd, but we used `new`.
            // Correct fix: Use `free` and `malloc` for C-structs to be safe with void*.
            free(ptr);
        }
        layer_ptrs.clear();
        param_buffer.clear();
        weight_buffer.clear();
    }

    // Load from a byte buffer (e.g. read from LittleFS)
    bool load(const uint8_t* buffer, size_t len) {
        free_resources();
        
        if (len < 32) return false;
        
        const DynamicModelHeader* hdr = (const DynamicModelHeader*)buffer;
        if (hdr->magic != MODEL_MAGIC) return false;
        
        // 1. Point to Data Sections
        const uint8_t* data_ptr = buffer + 32;
        
        // 2. Sentinel God Mode Parser
        // We expect the specific layout generated by gen_sentinel_model.py
        // Input(128) -> Dense(128) -> ReLU -> Dense(5) -> Softmax
        
        // --- Layer 1: Input (128) ---
        input_shape[0] = 1; 
        input_shape[1] = 128; 
        
        ailayer_input_f32_t* input = (ailayer_input_f32_t*)malloc(sizeof(ailayer_input_f32_t));
        if (!input) return false;
        *input = AILAYER_INPUT_F32_A(2, input_shape); // Copy wrapper struct
        layer_ptrs.push_back(input);
        
        // --- Layer 2: Dense (128 -> 128) ---
        // Weights: 128*128 floats
        size_t w1_bytes = 128 * 128 * 4;
        size_t b1_bytes = 128 * 4;
        
        if (data_ptr + w1_bytes + b1_bytes > buffer + len) return false; // Buffer overflow check
        
        ailayer_dense_f32_t* d1 = (ailayer_dense_f32_t*)malloc(sizeof(ailayer_dense_f32_t));
        if (!d1) return false;
        *d1 = AILAYER_DENSE_F32_A(128);
        layer_ptrs.push_back(d1);
        
        // --- Layer 3: ReLU ---
        ailayer_relu_f32_t* r1 = (ailayer_relu_f32_t*)malloc(sizeof(ailayer_relu_f32_t));
        if (!r1) return false;
        *r1 = AILAYER_RELU_F32_A();
        layer_ptrs.push_back(r1);
        
        // --- Layer 4: Dense (128 -> 5) ---
        size_t w2_bytes = 128 * 5 * 4;
        size_t b2_bytes = 5 * 4;
        
        if (data_ptr + w1_bytes + b1_bytes + w2_bytes + b2_bytes > buffer + len) return false;
        
        ailayer_dense_f32_t* d2 = (ailayer_dense_f32_t*)malloc(sizeof(ailayer_dense_f32_t));
        if (!d2) return false;
        *d2 = AILAYER_DENSE_F32_A(5);
        layer_ptrs.push_back(d2);
        
        // --- Layer 5: Softmax ---
        ailayer_softmax_f32_t* sm = (ailayer_softmax_f32_t*)malloc(sizeof(ailayer_softmax_f32_t));
        if (!sm) return false;
        *sm = AILAYER_SOFTMAX_F32_A();
        layer_ptrs.push_back(sm);
        
        // Output Shape
        output_shape[0] = 1;
        output_shape[1] = 5;
        
        // --- Link Graph ---
        ailayer_t* x;
        model.input_layer = ailayer_input_f32_default(input);
        x = ailayer_dense_f32_default(d1, model.input_layer);
        x = ailayer_relu_f32_default(r1, x);
        x = ailayer_dense_f32_default(d2, x);
        model.output_layer = ailayer_softmax_f32_default(sm, x);
        
        // --- Allocate Parameter Memory ---
        // AIfES needs a working buffer for intermediate calculations
        uint32_t param_size = aialgo_sizeof_parameter_memory(&model);
        
        // Reserve vector space
        param_buffer.resize(param_size);
        aialgo_distribute_parameter_memory(&model, param_buffer.data(), param_size);
        
        // --- Copy Weights ---
        // Now that memory is distributed, we must copy the weights from the file buffer
        // into the pointers inside the layer structs (which now point into param_buffer).
        // Wait, `distribute_parameter_memory` sets the data pointers in the structs?
        // NO, it sets up the *internal* parameter buffers.
        // For F32, the weights/bias pointers in `ailayer_dense_f32_t` should point to
        // the allocated memory in `param_buffer`.
        
        // Let's verify: `ailayer_dense_f32_default` usually sets up the layer to *use* the
        // memory distributed later.
        
        // Actually, for F32, we usually init the weights structure to point to the data.
        // AIfES `distribute` handles the *inference* memory state, but for weights?
        // In "God Mode", we must manually copy the data to where the layer expects it.
        
        // Correction: AIfES `distribute_parameter_memory` assigns the `weights.data` and `bias.data`
        // pointers of each layer to locations inside the provided `param_mem`.
        // So AFTER `distribute`, `d1->weights.data` is valid and points to `param_buffer`.
        // We just need to memcpy into it!
        
        // 1. Copy W1
        float* d1_w_dst = (float*)d1->weights.data; 
        const float* d1_w_src = (const float*)data_ptr;
        memcpy(d1_w_dst, d1_w_src, w1_bytes);
        data_ptr += w1_bytes;
        
        // 2. Copy B1
        float* d1_b_dst = (float*)d1->bias.data;
        const float* d1_b_src = (const float*)data_ptr;
        memcpy(d1_b_dst, d1_b_src, b1_bytes);
        data_ptr += b1_bytes;
        
        // 3. Copy W2
        float* d2_w_dst = (float*)d2->weights.data;
        const float* d2_w_src = (const float*)data_ptr;
        memcpy(d2_w_dst, d2_w_src, w2_bytes);
        data_ptr += w2_bytes;
        
        // 4. Copy B2
        float* d2_b_dst = (float*)d2->bias.data;
        const float* d2_b_src = (const float*)data_ptr;
        memcpy(d2_b_dst, d2_b_src, b2_bytes);
        
        return true; 
    }
    
    aimodel_t* get_model() {
        return &model;
    }
    
    // Generic Inference
    void infer(float* input_data, float* output_data) {
       // We need an inference memory buffer
       // For this simple demo, we alloc on stack or reuse a small buffer
       // Ideally this should be computed
       uint8_t infer_mem[2048]; // Enough for this small model? 128*sizeof(float) is 512 bytes.
       
       // Wrap data
       aitensor_t input_tensor = AITENSOR_2D_F32(input_shape, input_data);
       aitensor_t output_tensor = AITENSOR_2D_F32(output_shape, output_data);
       
       aialgo_schedule_inference_memory(&model, infer_mem, sizeof(infer_mem));
       aialgo_inference_model(&model, &input_tensor, &output_tensor);
    }
};

#endif // SPRITE_DYNAMIC_H
